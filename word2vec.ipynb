{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom training for Word2Vec : \n",
    "word2vec is nothing but a static embedding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the all files from the nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sent_tokenizer from nltk and\n",
    "# importing simple_preprocess from gensim.utils\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one empty list for storing all the sentence_tokens that will be collected from the dataset\n",
    "story=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir('data'):\n",
    "  file_path=os.path.join('data',file_name)\n",
    "  with open(file_path,encoding='unicode_escape') as f:\n",
    "    corpus=f.read()\n",
    "  raw_sent=sent_tokenize(corpus)\n",
    "  for sent in raw_sent:\n",
    "    story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the data through story list\n",
    "story[:2]\n",
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are importing Word2Vec() class from gensim.models\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation of Word2Vec() function and passing few arguments\n",
    "model=Word2Vec(\n",
    "    window=10,\n",
    "    min_count=5,\n",
    "    vector_size=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will call the model and passing the whole dataset that\n",
    "# are stored into the story list\n",
    "# build_vocab() function creates unique vocabulary\n",
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing few attribute for the word2vec model\n",
    "model.corpus_count\n",
    "model.epochs\n",
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will train our model\n",
    "model.train(\n",
    "    story,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=model.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check the vector of the word 'king' through this model\n",
    "model.wv['king']\n",
    "\n",
    "# checking the dimension of the king's vector\n",
    "len(model.wv['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the most similar word of 'king' through most_similar() function\n",
    "model.wv.most_similar('king')\n",
    "model.wv.most_similar('daenerys')\n",
    "model.wv.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using of doesnt_match() function\n",
    "model.wv.doesnt_match(['jon','rikon','robb','arya','sansa','bran'])\n",
    "model.wv.doesnt_match(['cersei','jaime','bronn','tyrion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing all the vectors in the form of Array\n",
    "model.wv.get_normed_vectors()\n",
    "\n",
    "# length of the normed vectors\n",
    "len(model.wv.get_normed_vectors())\n",
    "\n",
    "# shape of the vector\n",
    "model.wv.get_normed_vectors().shape\n",
    "\n",
    "# all the words of the vector\n",
    "model.wv.index_to_key\n",
    "\n",
    "# no of words for training\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model as .model extension\n",
    "model.save('/content/word2vec.model')\n",
    "\n",
    "# saving the model as .bin extension\n",
    "model.save('/content/word2vec.bin')\n",
    "\n",
    "# saving the model as .bin.gzip extension\n",
    "! gzip word2vec.bin > word2vec.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
