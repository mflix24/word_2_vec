{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom training for Word2Vec : \n",
    "word2vec is nothing but a static embedding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading the all files from the nltk\n",
    "# nltk.download('all')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sent_tokenizer from nltk and\n",
    "# importing simple_preprocess from gensim.utils\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one empty list for storing all the sentence_tokens that will be collected from the dataset\n",
    "story=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir('data'):\n",
    "  file_path=os.path.join('data',file_name)\n",
    "  with open(file_path,encoding='unicode_escape') as f:\n",
    "    corpus=f.read()\n",
    "  raw_sent=sent_tokenize(corpus)\n",
    "  for sent in raw_sent:\n",
    "    story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141218"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the data through story list\n",
    "story[:2]\n",
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are importing Word2Vec() class from gensim.models\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation of Word2Vec() function and passing few arguments\n",
    "model=Word2Vec(\n",
    "    window=10,\n",
    "    min_count=5,\n",
    "    vector_size=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will call the model and passing the whole dataset that\n",
    "# are stored into the story list\n",
    "# build_vocab() function creates unique vocabulary\n",
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1725638"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing few attribute for the word2vec model\n",
    "model.corpus_count\n",
    "model.epochs\n",
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6482182, 8628190)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will train our model\n",
    "model.train(\n",
    "    story,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=model.epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will check the vector of the word 'king' through this model\n",
    "model.wv['king']\n",
    "\n",
    "# checking the dimension of the king's vector\n",
    "len(model.wv['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.736301839351654),\n",
       " ('margaery', 0.7111654877662659),\n",
       " ('daenerys', 0.6761711239814758),\n",
       " ('cersei', 0.6538811326026917),\n",
       " ('joffrey', 0.6476494669914246),\n",
       " ('mother', 0.6363057494163513),\n",
       " ('prince', 0.6259520649909973),\n",
       " ('myrcella', 0.6250472068786621),\n",
       " ('stormborn', 0.6177440881729126),\n",
       " ('elia', 0.5968142747879028)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding out the most similar word of 'king' through most_similar() function\n",
    "model.wv.most_similar('king')\n",
    "model.wv.most_similar('daenerys')\n",
    "model.wv.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bronn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using of doesnt_match() function\n",
    "model.wv.doesnt_match(['jon','rikon','robb','arya','sansa','bran'])\n",
    "model.wv.doesnt_match(['cersei','jaime','bronn','tyrion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11760"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing all the vectors in the form of Array\n",
    "model.wv.get_normed_vectors()\n",
    "\n",
    "# length of the normed vectors\n",
    "len(model.wv.get_normed_vectors())\n",
    "\n",
    "# shape of the vector\n",
    "model.wv.get_normed_vectors().shape\n",
    "\n",
    "# all the words of the vector\n",
    "model.wv.index_to_key\n",
    "\n",
    "# no of words for training\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model as .model extension\n",
    "model.save('word2vec.model')\n",
    "\n",
    "# saving the model as .bin extension\n",
    "model.save('word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
